# -*- coding:utf-8 -*-
import requests
import sqlite3
from bs4 import BeautifulSoup
import re
import urlparse
from os import path

url = "https://www.exploit-db.com/papers/?order_by=date&order=desc&pg=%s&size=0"
headers = {
        "user-agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/52.0.2743.116 Safari/537.36"
    }

def download(conn, total_page, force):
    cur = conn.cursor()

    exist_ids = set((r[0] for r in cur.execute("select id from paper")))

    session = requests.session()

    for page in xrange(1, total_page+1):
        print "-----start download page: %s-----" % (page, )
        res = session.get(url % (page, ), headers=headers)
        soup = BeautifulSoup(res.text, 'html.parser')
        for row in soup.select(".exploit_list tbody tr"):
            download_link = row.select_one(".dlink a").get("href")
            paper_id = download_link.split("/")[-1].split(".")[0]

            if not force and paper_id in exist_ids:
                print "update to last download", paper_id
                return
            date = row.select_one(".date").string.strip(" \n")
            title = row.select_one(".description a").string.strip()
            author = row.select_one(".author a")
            author_name = author.string.strip()
            author_id = author.get("href").split("=")[1]

            if not download_link.startswith("http"):
                download_link = "https://www.exploit-db.com"+download_link

            print "download:", (paper_id, date, title, author_id, author_name, download_link)

            try:
                r = session.get(download_link, headers=headers, stream=True)
                _, _, path_, _, _ = urlparse.urlsplit(download_link)
                path_list = path_.split(".")
                if len(path_list) == 2:
                    file_type = path_list[-1]
                else:
                    file_type = r.headers["content-type"].split("/")[-1]
                print "file_type:", file_type

                file_path = "./docs/"+paper_id+"."+file_type
                if path.exists(file_path) and paper_id in exist_ids:
                    print "already download:", paper_id
                    continue

                with open(file_path, 'wb') as f:
                    for chunk in r.iter_content(chunk_size=1024):
                        if chunk:
                            f.write(chunk)
                            f.flush()
                cur.execute("insert into paper values (?, ?, ?, ?, ?, ?, ?)",
                                (paper_id, date, title, author_id, author_name, download_link, file_type))
                conn.commit()
            except sqlite3.IntegrityError, e:
                print "re download:", paper_id
            except Exception, e:
                print e


def main():
    total_pattern = re.compile("[0-9,]+")
    page_pattern = re.compile("pg=([0-9]+)")

    conn = sqlite3.connect('exploit.db')

    res_ = requests.get(url % (1, ), headers=headers)
    soup = BeautifulSoup(res_.text, 'html.parser')
    pagination = soup.find("div", class_="pagination")
    total = int(total_pattern.findall(pagination.find("div").string)[0].replace(",",""))
    total_page = int(page_pattern.search(pagination.find_all("a")[-1].get("href")).group(1))
    print "now exploit-db has %s papers, %s page" % (total, total_page)
    download(conn, total_page, True)
    conn.close()



if __name__ == '__main__':
    main()